{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ldkQYasRMgOY"},"source":["# Problems 7\n"]},{"cell_type":"markdown","metadata":{"id":"KmpETZuVMgOc"},"source":["Name: Denitsa Ilieva"]},{"cell_type":"code","metadata":{"id":"s4NOt_qdMgOf"},"source":["# imports\n","import numpy as np\n","np.random.seed(123)\n","import pandas as pd\n","\n","# Fill in any place that says 'Your code here'.\n","# You can implement auxiliary methods if you deem it proper. You may also change or extend functions, but:\n","# Make sure the code blocks at the end of each problem run as intended!!!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X4dzMkHIMgOh"},"source":["***\n","\n","\n","# Problem 1\n","\n","Regularization can be formalized by adding a weighted regularizer to a loss function, leading to the optimization problem:\n","\n","$$\\arg\\!\\min_{w} \\mathcal{L}(\\mathcal{T}; w) + \\lambda \\mathcal{R}_p(w)$$\n","\n","where $\\mathcal{R}_p(w) = \\sum_{i=1}^n |w_i|^p, \\; \\lambda > 0$.\n","\n","1. How is the $\\mathcal{R}_0$ regularizer defined and what is its effect?\n","1. How is the $\\mathcal{R}_1$ regularizer defined and what is its effect?\n","1. How is the $\\mathcal{R}_2$ regularizer defined and what is its effect?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3pn3BcrZMgOi"},"source":["YOUR ANSWER HERE\n","\n","$$\n","\\nabla \\omega \\mathcal{R}_0(ω) = 0 \\cdot \\left(\n","  \\begin{array}{ll}\n","  sign(ω_1* |ω_1|^{-1})\\\\\n","  ...\\\\\n","  sign(ω_n*|ω_n|^{-1})\n","  \\end{array}\n","\\right) = 0\n","$$\n","<br>\n","With $\\mathcal{R}_0$ there is no effect, because the multiplicated weight vector with 0 outputs 0\n","<br>\n","Counts non-zero weights. Minimization only possible by setting weights to 0 <br>\n","$$\n","\\sum_{i=1} |ω_i|^0 = \\sum_{i=1} [\\omega_i >0]\n","$$\n","<br>\n","$$\n","\\nabla \\omega \\mathcal{R}_1(ω) = 1 \\cdot \\left(\n","  \\begin{array}{ll}\n","  sign(ω_1* |ω_1|^{0})\\\\\n","  ...\\\\\n","  sign(ω_n*|ω_n|^{0})\n","  \\end{array}\n","\\right) = \\left(\n","  \\begin{array}{ll}\n","  sign(ω_1)\\\\\n","  ...\\\\\n","  sign(ω_n)\n","  \\end{array}\n","\\right)\n","$$\n","<br>\n","$\\mathcal{R}_1$ adds “absolute value of magnitude” of coefficient as penalty term to the loss function. This regulizer shrinks the less important feature’s coefficient to zero.\n","<br>\n","$L_{1}$ norm: sums absolute values. Minimization requires setting weights to 0\n","<br>\n","Laplace-distribution: sehr gute Wahrscheinlichkeit bei Werten, die sehr klein sind (neigend zu 0)\n","\n","$$\n","\\nabla \\omega \\mathcal{R}_2(ω) = 2 \\cdot \\left(\n","  \\begin{array}{ll}\n","  sign(ω_1* |ω_1|)\\\\\n","  ...\\\\\n","  sign(ω_n*|ω_n|)\n","  \\end{array}\n","\\right) = 2 ω_i\n","$$\n","<br>\n","$\\mathcal{R}_2$ adds “squared magnitude” of coefficient as penalty term to the loss function. If lambda is very large then it will add too much weight and it will lead to underfitting. If lambda is chosen appropriate then $\\mathcal{R}_2$ can avoid overfitting.\n","<br>\n","$L_{2}$ norm : sums squared values. Minimization possible by setting small values, through not necessarily 0 values\n","<br>\n","Normalverteilung\n"]},{"cell_type":"markdown","metadata":{"id":"0HDSW45yMgOi"},"source":["***\n","\n","# Problem 2\n","\n","Modify the perceptron update by adding a learning rate $\\alpha > 0$ to the update\n","\n","$$w = w + \\alpha \\left( \\phi(x_t, y_t) - \\phi(x_t, y) \\right)$$\n","\n","Implement this change and perform experiments with different learning rates $\\alpha$ on the iris data."]},{"cell_type":"code","metadata":{"id":"lMyS1_RoMgOj"},"source":["label_names = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n","\n","\n","def multiclass_perceptron(training_data, learning_rate, epochs):\n","    \"\"\"\n","    Trains a Perceptron model for multiclass classification.\n","\n","    Args:\n","    training_data -- list -- A list of all document vectors with their corresponding label.\n","    learning_rate -- float -- The training step.\n","    epochs -- integer -- the number of epochs\n","\n","    Returns:\n","    weights -- list/np.array -- The weights vector, which represents the trained Perceptron model.\n","    \"\"\"\n","    # YOUR CODE HERE\n","    weights = np.zeros(12)\n","    for epoch in range(epochs):\n","        for i in training_data:\n","            #print('i is', i)\n","            predicted_label = label_max_score(i[0], weights)\n","            if i[1] != predicted_label:\n","                update = learning_rate * (phi(i[0], i[1]) - phi(i[0], predicted_label))\n","                weights += update\n","    #print(weights)\n","    return weights\n","\n","def read_data(filename):\n","    \"\"\"\n","    Reads an annotated corpus into a list.\n","\n","    Args:\n","    filename -- str -- The name of the corpus file.\n","\n","    Returns:\n","    documents -- list -- A list of all documents with their corresponding label.\n","    \"\"\"\n","    # YOUR CODE HERE\n","    df = pd.read_csv(filename,header=None)\n","\n","    df[4] = df[4].replace(['Iris-setosa'],0)\n","    df[4] = df[4].replace(['Iris-versicolor'],1)\n","    df[4] = df[4].replace(['Iris-virginica'],2)\n","\n","    return df\n","\n","def get_vectors(data):\n","    \"\"\"\n","    Cleans up the document representations.\n","    ['5.1', '3.5', '1.4', '0.2', 'Iris-setosa'] -> [[5.1, 3.5, 1.4, 0.2], 0]\n","    Args:\n","    data -- list -- A list of all documents with their corresponding label.\n","    Returns:\n","    vectors -- list -- A list of all document vectors with their corresponding label.\n","    \"\"\"\n","    # YOUR CODE HERE ---> it's implemented in read_data() and split_train_test()\n","    pass\n","\n","\n","def perceptron_accuracy(test_data, weights):\n","    \"\"\"\n","    Computes the overall accuracy of the Perceptron model on the test data.\n","\n","    Args:\n","    test_data -- list/np.array -- A list of document vectors and their corresponding labels.\n","    weights -- list/np.array -- The weights vector, which represents the trained Perceptron model.\n","    \"\"\"\n","    # YOUR CODE HERE\n","    correct, incorrect = 0, 0\n","    for row in test_data:\n","        label0 = phi(row[0], 0.0)\n","        label1 = phi(row[0], 1.0)\n","        label2 = phi(row[0], 2.0)\n","        phi_list = [label0, label1, label2]\n","        vec = [0, 0, 0]\n","        for i in range(len(vec)):\n","            vec[i] = np.dot(weights, phi_list[i])\n","        predicted = vec.index(max(vec))\n","        #print('the predicted value is',predicted)\n","        #print('real value is', row[1])\n","        if predicted == row[1]:\n","            correct += 1\n","        else:\n","          incorrect += 1\n","    accuracy = (correct * 1.0) / ((correct + incorrect) * 1.0)\n","    return accuracy\n","\n","\n","def label_max_score(t, weights):\n","    \"\"\"\n","    Computes the overall accuracy of the Perceptron model on the test data.\n","\n","    Args:\n","    t -- list/np.array -- A data point (one training or test vector).\n","    weights -- list/np.array -- The weights vector, which represents the trained Perceptron model.\n","\n","    Returns:\n","    max_scoring_label -- integer -- The most probable class for instance t according to the model.\n","    \"\"\"\n","    # YOUR CODE HERE\n","    label0 = np.dot(phi(t, 0.0), weights)\n","    label1 = np.dot(phi(t, 1.0), weights)\n","    label2 = np.dot(phi(t, 2.0), weights)\n","    vec = [label0, label1, label2]\n","    predicted_label = vec.index(max(vec))\n","    return predicted_label\n","\n","\n","def phi(t, label):\n","    \"\"\"\n","    Block vector.\n","\n","    Args:\n","    t -- list/np.array -- A data point (one training or test vector).\n","    label -- integer -- The class.\n","\n","    Returns:\n","    block_vector -- list/np.array -- The pumped-up training instance t.\n","    \"\"\"\n","    # YOUR CODE HERE\n","    block_vector = np.zeros(12)\n","    for i in range(len(t)):\n","        idx = int((label * 4) + i)\n","        block_vector[idx] = t[i]\n","    return block_vector\n","\n","def split_train_test(data):\n","    \"\"\"\n","    Splits iris.data into training and test sets.\n","    \"\"\"\n","    # YOUR CODE HERE\n","    X = []\n","    y = []\n","    for index, rows in data.iterrows():\n","    # Create list for the current row\n","      if index%4==0:\n","        my_list =[[rows[0], rows[1], rows[2], rows[3]], rows[4]]\n","        y.append(my_list)\n","      else:\n","        my_list =[[rows[0], rows[1], rows[2], rows[3]], rows[4]]\n","        X.append(my_list)\n","    return X,y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MJh4IQPuEzt7"},"source":["iris = read_data(\"iris.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pcYEC5J1E4N9"},"source":["training_data, test_data = split_train_test(iris)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TxfBGKu0nTTP","executionInfo":{"status":"ok","timestamp":1638866306800,"user_tz":-60,"elapsed":16164,"user":{"displayName":"Denitsa Ilieva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10873287225646224114"}},"outputId":"19eeddf5-9136-4536-8fd5-fd0b9e4d2790"},"source":["lr = [0.0001, 0.001, 0.01, 0.1, 0.2]\n","for i in lr:\n","  weights = multiclass_perceptron(training_data, i, 2000)\n","  accur = perceptron_accuracy(test_data, weights)\n","  print('For the learning rate {:.4f}, the accuracy is {:.3f}'.format(i,accur),'\\n')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["For the learning rate 0.0001, the accuracy is 0.921 \n","\n","For the learning rate 0.0010, the accuracy is 0.921 \n","\n","For the learning rate 0.0100, the accuracy is 0.921 \n","\n","For the learning rate 0.1000, the accuracy is 0.921 \n","\n","For the learning rate 0.2000, the accuracy is 0.921 \n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"QW_qJRMfCj_Y"},"source":["We see that the accuracy remains the same for the selected learning rates .\n","\n"]},{"cell_type":"markdown","metadata":{"id":"o40HsVuqMgOm"},"source":["***\n","\n","# Problem 3\n","\n","For the perceptron algorithm, adding a weighted $\\mathcal{R}_2$ regularizer amounts to modifying the update to\n","\n","$$w = w + \\alpha \\left( \\phi(x_t, y_t) - \\phi(x_t, y) - \\lambda w \\right)$$\n","\n","Implement this change and vary regularization strength by adjusting $\\lambda$ on the iris data."]},{"cell_type":"code","metadata":{"id":"Zi0KLe-dMgOn"},"source":["def multiclass_perceptron(training_data, regularization, learning_rate=0.1, epochs = 2000):\n","    \"\"\"\n","    Trains a Perceptron model for multiclass classification.\n","\n","    Args:\n","    training_data -- list -- A list of all document vectors with their corresponding label.\n","    learning_rate -- float -- The training step.\n","    regularization -- float -- The regularization parameter.\n","    epochs -- integer -- the number of epochs\n","\n","    Returns:\n","    weights -- list/np.array -- The weights vector, which represents the trained Perceptron model.\n","    \"\"\"\n","    # YOUR CODE HERE\n","    weights = np.zeros(12)\n","    for epoch in range(epochs):\n","        for i in training_data:\n","            predicted_label = label_max_score(i[0], weights)\n","            if i[1] != predicted_label:\n","                update = learning_rate * ((phi(i[0], i[1]) - phi(i[0], predicted_label)) - (regularization*weights))\n","                weights += update\n","    return weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Ybk8jqWsDSA","executionInfo":{"status":"ok","timestamp":1638743255510,"user_tz":-60,"elapsed":21979,"user":{"displayName":"Denitsa Ilieva","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10873287225646224114"}},"outputId":"72dccc3b-ae6d-4a0e-b948-1e08d081d1ea"},"source":["regulariser = [0.1, 0.25, 0.5, 0.75, 1, 1.5, 2]\n","for i in regulariser:\n","  weights = multiclass_perceptron(training_data, i, 0.1, 2000)\n","  accur = perceptron_accuracy(test_data, weights)\n","  print('For the regularization {:.2f}, the accuracy is {:.3f}'.format(i,accur),'\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["For the regularization 0.10, the accuracy is 0.684 \n","\n","For the regularization 0.25, the accuracy is 0.368 \n","\n","For the regularization 0.50, the accuracy is 0.342 \n","\n","For the regularization 0.75, the accuracy is 0.684 \n","\n","For the regularization 1.00, the accuracy is 0.684 \n","\n","For the regularization 1.50, the accuracy is 0.684 \n","\n","For the regularization 2.00, the accuracy is 0.342 \n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"uyUChG0AMgOo"},"source":["YOUR ANSWER HERE\n","\n","The results are very different. For some minor regularization parameter, we consider underfitting. For the values between 0.75 and 1.5, the accuracy is increased, but still the model has not learned as well. For large regularization parameters, no overfitting is considered as with R2 regularizer. One reason for this could be the small data set.\n","<br>\n","Wenn der Regulizirer zu hoch ist, gibt es keinen Unterschied, wie viel Epochen angelegt sind -> bei NN hat der Regulisierer nicht so großen Effekt, weil diese Modelle sowieso zum Overfitting bringen\n","<br>\n","Lr ist von der Anzahl der Epochen abhängig"]},{"cell_type":"code","metadata":{"id":"IPbqawAwEIjd"},"source":[],"execution_count":null,"outputs":[]}]}